{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 K-Nearest Neighbors On Two Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleveland Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common imports for use in the data visualization\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,  precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandDf = pd.read_csv('datasets/cleveland.csv')\n",
    "\n",
    "clevelandDf.replace('?', np.nan, inplace=True)\n",
    "\n",
    "clevelandDf['num'] = (clevelandDf['num'] > 0).astype(int)\n",
    "\n",
    "clevelandDf.dropna(inplace=True)\n",
    "\n",
    "selected_features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "X = clevelandDf[selected_features]\n",
    "y = clevelandDf['num']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_k = 1  # test others\n",
    "nn = NearestNeighbors(n_neighbors=best_k, metric='euclidean', algorithm='auto')\n",
    "nn.fit(X_train)\n",
    "\n",
    "distances, indices = nn.kneighbors(X_test)\n",
    "predictions = [np.any(y_train.iloc[indices[i]] > 0) for i in range(len(X_test))]  # Considering any non-zero prediction as heart disease\n",
    "\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(\"K value:\", best_k)\n",
    "print(\"Selected attributes:\", selected_features)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As I messed around with different K values, I relaized that as I went up, we got worse. This is because of overfitting. As we made the model more complex than it needs to be, we started getting more and more incorrect. But as our K decreased, our scores went up, leading us to go with a lower K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('datasets/cleveland-test-sample.csv')\n",
    "\n",
    "new_data.replace('?', np.nan, inplace=True)\n",
    "new_data.dropna(inplace=True)\n",
    "X_new = new_data[selected_features]\n",
    "\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=best_k, metric='euclidean', algorithm='auto')\n",
    "nn.fit(X_scaled)\n",
    "distances, indices = nn.kneighbors(X_new_scaled)\n",
    "predictions = [np.any(y.iloc[indices[i]] > 0) for i in range(len(X_new_scaled))]  # Consider any non-zero prediction as heart disease\n",
    "\n",
    "print(\"Predictions for new dataset:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Student Dropout Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Good Datasets Here](https://archive.ics.uci.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "school_df = pd.read_csv('datasets/data.csv', sep=';')\n",
    "interested_columns = [\"Age at enrollment\", \"Tuition fees up to date\", \"Previous qualification (grade)\", \"Scholarship holder\", \"Target\"]\n",
    "school_interested = school_df[interested_columns].copy()\n",
    "\n",
    "dropout_count = school_interested['Target'].value_counts().get('Dropout')\n",
    "print(\"Number of dropouts:\", dropout_count)\n",
    "not_dropout_count = (school_interested['Target'] != 'dropout').sum()\n",
    "print(\"Number of students who are not dropouts:\", not_dropout_count)\n",
    "\n",
    "\n",
    "# Standardize the non-binary data\n",
    "school_interested['Age standardized'] = (school_interested['Age at enrollment'] - school_interested['Age at enrollment'].mean()) / (school_interested['Age at enrollment'].std())\n",
    "school_interested['Tuition standardized'] = (school_interested['Tuition fees up to date'] - school_interested['Tuition fees up to date'].mean()) / (school_interested['Tuition fees up to date'].std())\n",
    "school_interested['Grades standardized'] = (school_interested['Previous qualification (grade)'] - school_interested['Previous qualification (grade)'].mean()) / (school_interested['Previous qualification (grade)'].std())\n",
    "school_interested['Scholarship standardized'] = (school_interested['Scholarship holder'] - school_interested['Scholarship holder'].mean()) / (school_interested['Scholarship holder'].std())\n",
    "\n",
    "school_interested = school_interested.sort_values(by='Scholarship holder', ascending=False)\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=9, metric='euclidean', algorithm='auto')\n",
    "\n",
    "student = school_interested.sample(1)\n",
    "studentX = student[['Age standardized', 'Grades standardized']].values[0]\t\t# moved this block above\n",
    "display(studentX)\n",
    "\n",
    "# need to get rid of the patient itself\n",
    "X = school_interested.drop(student.index)[['Age standardized', 'Grades standardized']].values\n",
    "\n",
    "# Build index data structure under the hood for query performance\n",
    "fit = nn.fit(X)\n",
    "\n",
    "# Find k nearest neighbors\n",
    "distances, indices = fit.kneighbors([studentX])\n",
    "distances, indices\n",
    "\n",
    "# Get the patients that are near the age\n",
    "nbrs = school_interested.iloc[indices[0]]\n",
    "display(nbrs)\n",
    "\n",
    "# Print how many patients are sick and how many are healthy\n",
    "in_school = nbrs[(nbrs['Target'] == \"Enrolled\") | (nbrs['Target'] == \"Graduate\")].count()\n",
    "dropout = nbrs[nbrs['Target'] == \"Dropout\"].count()\n",
    "#print('in school: {}\\ndropout: {}'.format(in_school, dropout))\n",
    "\n",
    "prediction = \"isn't a dropout\" if in_school['Target'] > dropout['Target'] else \"is a dropout\"\n",
    "print(\"We predict this student is: \" + prediction)\n",
    "\n",
    "#display(school_interested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue here\n",
    "\n",
    "(p,r,f,s) = precision_recall_fscore_support(patientsy, y_pred, labels=[0,1])\n",
    "print(f'precision={p}, recall={r}, f-score={f}, support={s}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
